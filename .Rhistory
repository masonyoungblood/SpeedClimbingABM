plot(model_b[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", 0.1))
for(i in 2:100){lines(model_b[[i]], col = scales::alpha("black", 0.1))}
#reinforcement learning with full memory, when only the used strategy is updated
model_c <- mclapply(1:100, function(x){model(50, 50, n_prefs = 10, neg_cost = 0, list(n = 1, rho = 1, phi = 1, delta = 0, lambda = 4), plot = FALSE)}, mc.cores = 7)
par(mar = c(4, 4, 1, 1))
plot(model_c[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", 0.1))
for(i in 2:100){lines(model_c[[i]], col = scales::alpha("black", 0.1))}
#load libraries
library(parallel)
#parameters for parallelization
iter <- 100
cores <- 7
#basic reinforcement learning, when only the used strategy is updated, with no negotiation cost
model_a <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0, list(n = 1, rho = 0, phi = 0, delta = 0, lambda = 4), plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_a[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", 0.1))
for(i in 2:iter){lines(model_a[[i]], col = scales::alpha("black", 0.1))}
#reinforcement learning with partial memory, when only the used strategy is updated, with no negotiation cost
model_b <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0, list(n = 1, rho = 0.5, phi = 0.5, delta = 0, lambda = 4), plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_b[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", 0.1))
for(i in 2:iter){lines(model_b[[i]], col = scales::alpha("black", 0.1))}
#basic reinforcement learning, when only the used strategy is updated, with no negotiation cost
model_c <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0.5, list(n = 1, rho = 0, phi = 0, delta = 0, lambda = 4), plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_c[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", 0.1))
for(i in 2:iter){lines(model_c[[i]], col = scales::alpha("black", 0.1))}
#reinforcement learning with partial memory, when only the used strategy is updated, with no negotiation cost
model_d <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0.5, list(n = 1, rho = 0.5, phi = 0.5, delta = 0, lambda = 4), plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_d[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", 0.1))
for(i in 2:iter){lines(model_d[[i]], col = scales::alpha("black", 0.1))}
#ewa attraction function
#if simple = TRUE (default) then the simplified version of EWA is used
attraction <- function(a, n = NULL, phi, delta = NULL, i = NULL, pi, rho = NULL, simple = TRUE){
if(simple){
(1 - phi)*a + phi*pi
} else{
(phi*n*a + (delta + (1 - delta)*i)*pi)/observation(n, rho)
}
}
attraction(a = 1, phi = 1, pi = 1)
attraction(a = 1, phi = 1, pi = 1)
attraction(a = 1, phi = 1, pi = 0)
attraction(a = 3, phi = 1, pi = 0)
attraction(a = 3, phi = 1, pi = 0.5)
attraction(a = 3, phi = 0.5, pi = 0.5)
attraction(a = 3, phi = 0.5, pi = 0.5)
attraction(a = 3, phi = 0.5, pi = 0.5)
# NOTES -------------------------------------------------------------------
#https://youtu.be/QvYb4NcFKj8?t=2175
#https://www.uni-heidelberg.de/md/awi/forschung/ewa_lecture.pdf
#possible improvement and simplificatin of original EWA: https://doi.org/10.1016/j.jet.2005.12.008
# FUNCTIONS ---------------------------------------------------------------
#define coordination game function
#a is focal individual, who's payoff is calculated twice against b: assuming a is a negotiator and assuming a is a conservative
#if status_quo = NULL, most common variant is chosen
coord_game <- function(a, b, status_quo = NULL, neg_cost, prefs, data){
#first assume a is negotiator, then assume a is conservative
payoffs <- sapply(1:2, function(x){
#if x is 1, then assume a is a negotiator, otherwise, assume a is a conservative
if(x == 1){
#store the conservatives, assuming a is not one
conservatives <- which(c(0, data$strat[b]) == 1)
} else{
#store the conservatives, assuming a is one
conservatives <- which(c(1, data$strat[b]) == 1)
}
#if both are conservatives
if(length(conservatives) == 2){
#store status quo (by default most common) move in both positions
if(is.null(status_quo)){
moves <- rep(names(which.max(table(data$pref))), 2)
} else{
moves <- rep(status_quo, 2)
}
#return payoff for a
return(data$payoffs[[a]][which(prefs == moves[1])])
}
#if only one is a conservative, (temporarily) do the same as if both are conservatives
if(length(conservatives) == 1){
#store status quo (by default most common) move in both positions
if(is.null(status_quo)){
moves <- rep(names(which.max(table(data$pref))), 2)
} else{
moves <- rep(status_quo, 2)
}
#return payoff for a
return(data$payoffs[[a]][which(prefs == moves[1])])
}
#if neither are conservatives, then enter the negotiation phase
if(length(conservatives) == 0){
#if preferred moves are the same
if(data$pref[a] == data$pref[b]){
#store moves in both positions
moves <- data$pref[c(a, b)]
#return payoff for a, with negotiation cost subtracted
return(data$payoffs[[a]][which(prefs == moves[1])] - neg_cost)
}
#if preferred moves are not the same
if(data$pref[a] != data$pref[b]){
#store move from agent with highest bargaining power in both positions
moves <- rep(data$pref[c(a, b)][which.max(data$power[c(a, b)])], 2)
#return payoff for a, with negotiation cost subtracted
return(data$payoffs[[a]][which(prefs == moves[1])] - neg_cost)
}
}
})
#return output
return(payoffs)
}
#ewa observation function, to update the number of previous rounds of experience, depreciated by rho
observation <- function(n, rho){rho*n + 1}
#ewa attraction function
#if simple = TRUE (default) then the simplified version of EWA is used
attraction <- function(a, n = NULL, phi, delta = NULL, i = NULL, pi, rho = NULL, simple = TRUE){
if(simple){
(1 - phi)*a + phi*pi
} else{
(phi*n*a + (delta + (1 - delta)*i)*pi)/observation(n, rho)
}
}
#ewa softmax function, to give probability of being a negotiator (0) in the next round
#needs a_0 (attractiveness of negotiator strategy) and a_1 (attractiveness of conservative strategy) as arguments
softmax <- function(a_0, a_1, lambda){exp(lambda*a_0)/(exp(lambda*a_0) + exp(lambda*a_1))}
#full ewa function, which requires current attraction and payoff of both strategies, outputs probability of being a negotiator (0)
ewa <- function(a_0, a_1, n = NULL, phi, delta = NULL, strat_used = NULL, pi_0, pi_1, rho = NULL, lambda, simple = TRUE){
#update attractions of a_0 and a_1 based on payoffs (pi)
if(simple){
a_0_new <- attraction(a = a_0, phi = phi, pi = pi_0)
a_1_new <- attraction(a = a_1, phi = phi, pi = pi_1)
} else{
a_0_new <- attraction(a = a_0, n, phi, delta, ifelse(strat_used == 0, 1, 0), pi_0, rho, simple = FALSE)
a_1_new <- attraction(a = a_1, n, phi, delta, ifelse(strat_used == 1, 1, 0), pi_1, rho, simple = FALSE)
}
#update (global) n
if(!simple){n_new <- observation(n, rho)}
#get probability of 0 based on new attractions
prob_0 <- softmax(a_0_new, a_1_new, lambda)
#return everything in named list
if(simple){
return(list(a_0 = a_0_new, a_1 = a_1_new, prob_0 = prob_0))
} else{
return(list(a_0 = a_0_new, a_1 = a_1_new, n_new = n_new, prob_0 = prob_0))
}
}
#full model function
#ewa_params requires named list of the following parameters:
#n (prior for n)
#rho (rate at which previous information is depreciated) (0-1)
#phi (weight given to new experience) (0-1)
#delta (whether only used strategy is updated (0) or used and unused strategies are updated (1)) (0-1)
#lambda (randomness in softmax, where 0 is completely random and larger values are more deterministic)
#model collapses to reinforcement learning when n = 1, rho = 0, and phi = 0
model <- function(pop_size, t, n_prefs, neg_cost, ewa_params, simple = TRUE, plot = TRUE){
#store letters of each pref
prefs <- LETTERS[1:n_prefs]
#initalize population of agents, all of whom are negotiators at the beginning
agents <- data.table::data.table(pref = NA, strat = 0,
payoffs = lapply(1:pop_size, function(x){rnorm(n_prefs)}),
power = sapply(1:pop_size, function(x){rnorm(1)}),
a_0 = 0, a_1 = 0)
#add in preferences based on sampled payoffs
agents$pref <- sapply(1:pop_size, function(x){prefs[which.max(agents$payoffs[[x]])]})
#store status quo
#status_quo <- sample(prefs, 1) #arbitrary preference
status_quo <- NULL #most common preference
#create output object to store proportion of conservatives in each timestep
output <- c()
for(i in 1:t){
#generate data frame of duos to coordinate (fully-connected population)
duos <- data.frame(x = sample(1:nrow(agents)), y = NA)
duos$y <- sapply(duos$x, function(x){sample(duos$x[-x], 1)})
#make new copy of agents that can be iteratively modified (t+1) while agents interact (t)
agents_new <- agents
for(j in 1:nrow(duos)){
#calculate payoffs from coordination game for individual a and individual b
payoffs_a <- coord_game(duos[j, 1], duos[j, 2], status_quo, neg_cost, prefs, agents)
payoffs_b <- coord_game(duos[j, 2], duos[j, 1], status_quo, neg_cost, prefs, agents)
#solve EWA for first individual in duos
if(simple){
ewa_out_a <- ewa(a_0 = agents$a_0[duos[j, 1]], a_1 = agents$a_1[duos[j, 1]], phi = ewa_params$phi,
pi_0 = payoffs_a[1], pi_1 = payoffs_a[2], lambda = ewa_params$lambda)
} else{
ewa_out_a <- ewa(a_0 = agents$a_0[duos[j, 1]], a_1 = agents$a_1[duos[j, 1]], n, phi, delta,
strat_used = agents$strat[duos[j, 1]], pi_0 = payoffs_a[1], pi_1 = payoffs_a[2], rho, lambda, simple = FALSE)
}
#solve EWA for second individual in duos
if(simple){
ewa_out_a <- ewa(a_0 = agents$a_0[duos[j, 2]], a_1 = agents$a_1[duos[j, 2]], phi = ewa_params$phi,
pi_0 = payoffs_b[1], pi_1 = payoffs_b[2], lambda = ewa_params$lambda)
} else{
ewa_out_a <- ewa(a_0 = agents$a_0[duos[j, 2]], a_1 = agents$a_1[duos[j, 2]], n, phi, delta,
strat_used = agents$strat[duos[j, 2]], pi_0 = payoffs_b[1], pi_1 = payoffs_b[2], rho, lambda, simple = FALSE)
}
#overwrite a_0 and a_1 for first individual
agents_new$a_0[duos[j, 1]] <- ewa_out_a$a_0
agents_new$a_1[duos[j, 1]] <- ewa_out_a$a_1
#overwrite a_0 and a_1 for second individual
agents_new$a_0[duos[j, 2]] <- ewa_out_b$a_0
agents_new$a_1[duos[j, 2]] <- ewa_out_b$a_1
#sample new strategy for first individual
agents_new$strat[duos[j, 1]] <- sample(c(0, 1), 1, prob = c(ewa_out_a$prob_0, 1-ewa_out_a$prob_0))
#sample new strategy for second individual
agents_new$strat[duos[j, 2]] <- sample(c(0, 1), 1, prob = c(ewa_out_b$prob_0, 1-ewa_out_b$prob_0))
#if this is the final pair from the generation, then overwrite n with a new value
if(!simple){if(j == nrow(duos)){n <- ewa_out_a$n_new}}
#remove objects
rm(list = c("payoffs_a", "payoffs_b", "ewa_out_a", "ewa_out_b"))
}
#overwrite agents
agents <- agents_new
#store output
output <- c(output, sum(agents$strat)/nrow(agents))
#remove objects
rm(list = c("agents_new", "duos"))
}
#plot output
if(plot){plot(output, ylim = c(0, 1), ylab = "Proportion Conservative", xlab = "Timesteps", type = "l")}
#return output
return(output)
}
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 0, lambda = 4)
)
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 0, lambda = 4))
#full model function
#ewa_params requires named list of the following parameters:
#n (prior for n)
#rho (rate at which previous information is depreciated) (0-1)
#phi (weight given to new experience) (0-1)
#delta (whether only used strategy is updated (0) or used and unused strategies are updated (1)) (0-1)
#lambda (randomness in softmax, where 0 is completely random and larger values are more deterministic)
#model collapses to reinforcement learning when n = 1, rho = 0, and phi = 0
model <- function(pop_size, t, n_prefs, neg_cost, ewa_params, simple = TRUE, plot = TRUE){
#store letters of each pref
prefs <- LETTERS[1:n_prefs]
#initalize population of agents, all of whom are negotiators at the beginning
agents <- data.table::data.table(pref = NA, strat = 0,
payoffs = lapply(1:pop_size, function(x){rnorm(n_prefs)}),
power = sapply(1:pop_size, function(x){rnorm(1)}),
a_0 = 0, a_1 = 0)
#add in preferences based on sampled payoffs
agents$pref <- sapply(1:pop_size, function(x){prefs[which.max(agents$payoffs[[x]])]})
#store status quo
#status_quo <- sample(prefs, 1) #arbitrary preference
status_quo <- NULL #most common preference
#create output object to store proportion of conservatives in each timestep
output <- c()
for(i in 1:t){
#generate data frame of duos to coordinate (fully-connected population)
duos <- data.frame(x = sample(1:nrow(agents)), y = NA)
duos$y <- sapply(duos$x, function(x){sample(duos$x[-x], 1)})
#make new copy of agents that can be iteratively modified (t+1) while agents interact (t)
agents_new <- agents
for(j in 1:nrow(duos)){
#calculate payoffs from coordination game for individual a and individual b
payoffs_a <- coord_game(duos[j, 1], duos[j, 2], status_quo, neg_cost, prefs, agents)
payoffs_b <- coord_game(duos[j, 2], duos[j, 1], status_quo, neg_cost, prefs, agents)
#solve EWA for first individual in duos
if(simple){
ewa_out_a <- ewa(a_0 = agents$a_0[duos[j, 1]], a_1 = agents$a_1[duos[j, 1]], phi = ewa_params$phi,
pi_0 = payoffs_a[1], pi_1 = payoffs_a[2], lambda = ewa_params$lambda)
} else{
ewa_out_a <- ewa(a_0 = agents$a_0[duos[j, 1]], a_1 = agents$a_1[duos[j, 1]], n, phi, delta,
strat_used = agents$strat[duos[j, 1]], pi_0 = payoffs_a[1], pi_1 = payoffs_a[2], rho, lambda, simple = FALSE)
}
#solve EWA for second individual in duos
if(simple){
ewa_out_b <- ewa(a_0 = agents$a_0[duos[j, 2]], a_1 = agents$a_1[duos[j, 2]], phi = ewa_params$phi,
pi_0 = payoffs_b[1], pi_1 = payoffs_b[2], lambda = ewa_params$lambda)
} else{
ewa_out_b <- ewa(a_0 = agents$a_0[duos[j, 2]], a_1 = agents$a_1[duos[j, 2]], n, phi, delta,
strat_used = agents$strat[duos[j, 2]], pi_0 = payoffs_b[1], pi_1 = payoffs_b[2], rho, lambda, simple = FALSE)
}
#overwrite a_0 and a_1 for first individual
agents_new$a_0[duos[j, 1]] <- ewa_out_a$a_0
agents_new$a_1[duos[j, 1]] <- ewa_out_a$a_1
#overwrite a_0 and a_1 for second individual
agents_new$a_0[duos[j, 2]] <- ewa_out_b$a_0
agents_new$a_1[duos[j, 2]] <- ewa_out_b$a_1
#sample new strategy for first individual
agents_new$strat[duos[j, 1]] <- sample(c(0, 1), 1, prob = c(ewa_out_a$prob_0, 1-ewa_out_a$prob_0))
#sample new strategy for second individual
agents_new$strat[duos[j, 2]] <- sample(c(0, 1), 1, prob = c(ewa_out_b$prob_0, 1-ewa_out_b$prob_0))
#if this is the final pair from the generation, then overwrite n with a new value
if(!simple){if(j == nrow(duos)){n <- ewa_out_a$n_new}}
#remove objects
rm(list = c("payoffs_a", "payoffs_b", "ewa_out_a", "ewa_out_b"))
}
#overwrite agents
agents <- agents_new
#store output
output <- c(output, sum(agents$strat)/nrow(agents))
#remove objects
rm(list = c("agents_new", "duos"))
}
#plot output
if(plot){plot(output, ylim = c(0, 1), ylab = "Proportion Conservative", xlab = "Timesteps", type = "l")}
#return output
return(output)
}
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 0, lambda = 4))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 0, lambda = 4))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 0, lambda = 4))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 0, lambda = 4))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 1, lambda = 4))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 1, lambda = 4))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 1, lambda = 4))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 1, lambda = 4))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 1, lambda = 2))
model(50, 50, n_prefs = 10, neg_cost = 0, list(phi = 2, lambda = 2))
# NOTES -------------------------------------------------------------------
#https://youtu.be/QvYb4NcFKj8?t=2175
#https://www.uni-heidelberg.de/md/awi/forschung/ewa_lecture.pdf
#possible improvement and simplification of original EWA: https://doi.org/10.1016/j.jet.2005.12.008
#current version is based on expanded EWA in chapter 6: https://press.princeton.edu/books/hardcover/9780691090399/behavioral-game-theory
# FUNCTIONS ---------------------------------------------------------------
#define coordination game function
#a is focal individual, who's payoff is calculated twice against b: assuming a is a negotiator and assuming a is a conservative
#if status_quo = NULL, most common variant is chosen
coord_game <- function(a, b, status_quo = NULL, neg_cost, prefs, data){
#first assume a is negotiator, then assume a is conservative
payoffs <- sapply(1:2, function(x){
#if x is 1, then assume a is a negotiator, otherwise, assume a is a conservative
if(x == 1){
#store the conservatives, assuming a is not one
conservatives <- which(c(0, data$strat[b]) == 1)
} else{
#store the conservatives, assuming a is one
conservatives <- which(c(1, data$strat[b]) == 1)
}
#if both are conservatives
if(length(conservatives) == 2){
#store status quo (by default most common) move in both positions
if(is.null(status_quo)){
moves <- rep(names(which.max(table(data$pref))), 2)
} else{
moves <- rep(status_quo, 2)
}
#return payoff for a
return(data$payoffs[[a]][which(prefs == moves[1])])
}
#if only one is a conservative, (temporarily) do the same as if both are conservatives
if(length(conservatives) == 1){
#store status quo (by default most common) move in both positions
if(is.null(status_quo)){
moves <- rep(names(which.max(table(data$pref))), 2)
} else{
moves <- rep(status_quo, 2)
}
#return payoff for a
return(data$payoffs[[a]][which(prefs == moves[1])])
}
#if neither are conservatives, then enter the negotiation phase
if(length(conservatives) == 0){
#if preferred moves are the same
if(data$pref[a] == data$pref[b]){
#store moves in both positions
moves <- data$pref[c(a, b)]
#return payoff for a, with negotiation cost subtracted
return(data$payoffs[[a]][which(prefs == moves[1])] - neg_cost)
}
#if preferred moves are not the same
if(data$pref[a] != data$pref[b]){
#store move from agent with highest bargaining power in both positions
moves <- rep(data$pref[c(a, b)][which.max(data$power[c(a, b)])], 2)
#return payoff for a, with negotiation cost subtracted
return(data$payoffs[[a]][which(prefs == moves[1])] - neg_cost)
}
}
})
#return output
return(payoffs)
}
#ewa observation function, to update the number of previous rounds of experience
observation <- function(n, phi = 0, kappa = 0){phi*(1 - kappa)*n + 1}
#ewa attraction function
attraction <- function(a, n, phi, delta, i, pi, kappa){(phi*n*a + (delta + (1 - delta)*i)*pi)/observation(n, phi, kappa)}
#ewa softmax function, to give probability of being a negotiator (0) in the next round
#needs a_0 (attractiveness of negotiator strategy) and a_1 (attractiveness of conservative strategy) as arguments
softmax <- function(a_0, a_1, lambda){exp(lambda*a_0)/(exp(lambda*a_0) + exp(lambda*a_1))}
#full ewa function, which requires current attraction and payoff of both strategies, outputs probability of being a negotiator (0)
ewa <- function(a_0, a_1, n, phi, delta, strat_used, pi_0, pi_1, kappa, lambda){
#update attractions of a_0 and a_1 based on payoffs (pi)
a_0_new <- attraction(a_0, n, phi, delta, ifelse(strat_used == 0, 1, 0), pi_0, kappa)
a_1_new <- attraction(a_1, n, phi, delta, ifelse(strat_used == 1, 1, 0), pi_1, kappa)
#update (global) n
n_new <- observation(n, rho)
#get probability of 0 based on new attractions
prob_0 <- softmax(a_0_new, a_1_new, lambda)
#return everything named list
return(list(a_0 = a_0_new, a_1 = a_1_new, n_new = n_new, prob_0 = prob_0))
}
#full model function
#ewa parameters:
#n (prior for n)
#phi (weight given to new experience) (0-1)
#delta (whether only used strategy is updated (0) or used and unused strategies are updated (1)) (0-1)
#kappa (rate at which previous information is depreciated) (0-1)
#lambda (randomness in softmax, where 0 is completely random and larger values are more deterministic)
#special conditions:
#average reinforcement learning (default): delta = 0, kappa = 0, n = 1
#cumulative reinforcement learning: delta = 0, kappa = 1, n = 1
#weighted fictitious play: delta = 1, kappa = 0, n = 1
model <- function(pop_size, t, n_prefs, neg_cost, n = 1, phi, delta = 0, kappa = 0, lambda, plot = TRUE){
#store letters of each pref
prefs <- LETTERS[1:n_prefs]
#initalize population of agents, all of whom are negotiators at the beginning
agents <- data.table::data.table(pref = NA, strat = 0,
payoffs = lapply(1:pop_size, function(x){rnorm(n_prefs)}),
power = sapply(1:pop_size, function(x){rnorm(1)}),
a_0 = 0, a_1 = 0)
#add in preferences based on sampled payoffs
agents$pref <- sapply(1:pop_size, function(x){prefs[which.max(agents$payoffs[[x]])]})
#store status quo
#status_quo <- sample(prefs, 1) #arbitrary preference
status_quo <- NULL #most common preference
#create output object to store proportion of conservatives in each timestep
output <- c()
for(i in 1:t){
#generate data frame of duos to coordinate (fully-connected population)
duos <- data.frame(x = sample(1:nrow(agents)), y = NA)
duos$y <- sapply(duos$x, function(x){sample(duos$x[-x], 1)})
#make new copy of agents that can be iteratively modified (t+1) while agents interact (t)
agents_new <- agents
for(j in 1:nrow(duos)){
#calculate payoffs from coordination game for individual a and individual b
payoffs_a <- coord_game(duos[j, 1], duos[j, 2], status_quo, neg_cost, prefs, agents)
payoffs_b <- coord_game(duos[j, 2], duos[j, 1], status_quo, neg_cost, prefs, agents)
#solve EWA for first individual in duos
ewa_out_a <- ewa(a_0 = agents$a_0[duos[j, 1]], a_1 = agents$a_1[duos[j, 1]], n = n, phi = phi, delta = delta,
strat_used = agents$strat[duos[j, 1]], pi_0 = payoffs_a[1], pi_1 = payoffs_a[2], kappa = kappa, lambda = lambda)
#solve EWA for second individual in duos
ewa_out_b <- ewa(a_0 = agents$a_0[duos[j, 2]], a_1 = agents$a_1[duos[j, 2]], n = n, phi = phi, delta = delta,
strat_used = agents$strat[duos[j, 2]], pi_0 = payoffs_b[1], pi_1 = payoffs_b[2], kappa = kappa, lambda = lambda)
#overwrite a_0 and a_1 for first individual
agents_new$a_0[duos[j, 1]] <- ewa_out_a$a_0
agents_new$a_1[duos[j, 1]] <- ewa_out_a$a_1
#overwrite a_0 and a_1 for second individual
agents_new$a_0[duos[j, 2]] <- ewa_out_b$a_0
agents_new$a_1[duos[j, 2]] <- ewa_out_b$a_1
#sample new strategy for first individual
agents_new$strat[duos[j, 1]] <- sample(c(0, 1), 1, prob = c(ewa_out_a$prob_0, 1-ewa_out_a$prob_0))
#sample new strategy for second individual
agents_new$strat[duos[j, 2]] <- sample(c(0, 1), 1, prob = c(ewa_out_b$prob_0, 1-ewa_out_b$prob_0))
#if this is the final pair from the generation, then overwrite n with a new value
if(j == nrow(duos)){n <- ewa_out_a$n_new}
#remove objects
rm(list = c("payoffs_a", "payoffs_b", "ewa_out_a", "ewa_out_b"))
}
#overwrite agents
agents <- agents_new
#store output
output <- c(output, sum(agents$strat)/nrow(agents))
#remove objects
rm(list = c("agents_new", "duos"))
}
#plot output
if(plot){plot(output, ylim = c(0, 1), ylab = "Proportion Conservative", xlab = "Timesteps", type = "l")}
#return output
return(output)
}
model(50, 50, n_prefs = 10, neg_cost = 0, phi = 0, lambda = 4, plot = FALSE)
#full ewa function, which requires current attraction and payoff of both strategies, outputs probability of being a negotiator (0)
ewa <- function(a_0, a_1, n, phi, delta, strat_used, pi_0, pi_1, kappa, lambda){
#update attractions of a_0 and a_1 based on payoffs (pi)
a_0_new <- attraction(a_0, n, phi, delta, ifelse(strat_used == 0, 1, 0), pi_0, kappa)
a_1_new <- attraction(a_1, n, phi, delta, ifelse(strat_used == 1, 1, 0), pi_1, kappa)
#update (global) n
n_new <- observation(n, phi, kappa)
#get probability of 0 based on new attractions
prob_0 <- softmax(a_0_new, a_1_new, lambda)
#return everything named list
return(list(a_0 = a_0_new, a_1 = a_1_new, n_new = n_new, prob_0 = prob_0))
}
model(50, 50, n_prefs = 10, neg_cost = 0, phi = 0, lambda = 4, plot = FALSE)
model(50, 50, n_prefs = 10, neg_cost = 0, phi = 0, lambda = 4)
library(parallel)
#parameters for parallelization
iter <- 100
cores <- 7
#basic reinforcement learning, when only the used strategy is updated, with no negotiation cost
model_a <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0, phi = 0, lambda = 4, plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_a[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", 0.1))
for(i in 2:iter){lines(model_a[[i]], col = scales::alpha("black", 0.1))}
#reinforcement learning with partial memory, when only the used strategy is updated, with no negotiation cost
model_b <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0, phi = 0.5, lambda = 4, plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_b[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", 0.1))
for(i in 2:iter){lines(model_b[[i]], col = scales::alpha("black", 0.1))}
#load libraries
library(parallel)
#parameters for parallelization
iter <- 50
cores <- 7
alpha <- 0.2
#average RL model, with no negotiation cost
model_avg_rl <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0, phi = 0.5, delta = 0, kappa = 0, lambda = 4, plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_avg_rl[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", alpha))
for(i in 2:iter){lines(model_avg_rl[[i]], col = scales::alpha("black", alpha))}
#cumulative RL model, with no negotiation cost
model_cum_rl <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0, phi = 0.5, delta = 0, kappa = 1, lambda = 4, plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_cum_rl[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", alpha))
for(i in 2:iter){lines(model_cum_rl[[i]], col = scales::alpha("black", alpha))}
#weighted FP model, with no negotiation cost
model_wfp <- mclapply(1:iter, function(x){model(50, 50, n_prefs = 10, neg_cost = 0, phi = 0.5, delta = 1, kappa = 0, lambda = 4, plot = FALSE)}, mc.cores = cores)
par(mar = c(4, 4, 1, 1))
plot(model_wfp[[1]], type = "l", ylab = "Proportion Conservative", xlab = "Timesteps", ylim = c(0, 1), col = scales::alpha("black", alpha))
for(i in 2:iter){lines(model_wfp[[i]], col = scales::alpha("black", alpha))}
